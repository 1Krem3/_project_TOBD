"""
ETL Flow –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –æ –≤—ã–±—Ä–æ—Å–∞—Ö –≤ –∞—Ç–º–æ—Å—Ñ–µ—Ä—É
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Prefect –¥–ª—è –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ –∏ Dask –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
"""

import sqlite3
from datetime import datetime
from typing import Tuple, Optional
import pandas as pd
from prefect import flow, task, get_run_logger
from prefect_dask import DaskTaskRunner
import dask.dataframe as dd
from dask.distributed import Client, LocalCluster

db_path = "/home/user/Desktop/air-quality-project/data/air_emissions.db"
csv_path = "/home/user/Desktop/air-quality-project/data/air_emissions.csv"

# ============================================================================
# TASKS (–ó–∞–¥–∞—á–∏)
# ============================================================================

@task(name="extract_data", retries=2, retry_delay_seconds=30)
def extract_data(file_path: str = csv_path) -> dd.DataFrame:
    """
    –ó–∞–¥–∞—á–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV —Ñ–∞–π–ª–∞
    """
    logger = get_run_logger()
    logger.info(f"–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {file_path}")
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
        df = dd.read_csv(
            file_path,
            sep=';',
            encoding='utf-8',
            dtype=str,  # –í—Å–µ –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏
            on_bad_lines='skip',
            assume_missing=True
        )
        
        logger.info(f"–î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –°—Ç–æ–ª–±—Ü–æ–≤: {len(df.columns)}, –ø—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä: {len(df):,} —Å—Ç—Ä–æ–∫")
        logger.info(f"–°—Ç–æ–ª–±—Ü—ã: {list(df.columns)}")
        
        return df
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise

@task(name="transform_data")
def transform_data(df: dd.DataFrame) -> pd.DataFrame:
    """
    –ó–∞–¥–∞—á–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∏ –æ—á–∏—Å—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    """
    logger = get_run_logger()
    logger.info("–ù–∞—á–∞–ª–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö")
    
    try:
        # –®–∞–≥ 1: –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤
        new_names = [
            'section', 'indicator', 'unit', 'code', 'substance',
            'source_type', 'emission_type', 'location_level', 'region',
            'municipal_district', 'municipal_formation', 'oktmo_code',
            'year', 'value'
        ]
        df = df.rename(columns=dict(zip(df.columns, new_names)))
        logger.info("–°—Ç–æ–ª–±—Ü—ã –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω—ã")
        
        # –®–∞–≥ 2: –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ø—Ä–æ–ø—É—Å–∫–æ–≤
        logger.info("–£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤...")
        df_cleaned = df.dropna(subset=['value', 'section', 'code', 'substance'])
        
        # –®–∞–≥ 3: –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        logger.info("–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö...")
        if 'year' in df_cleaned.columns:
            df_cleaned['year'] = dd.to_numeric(df_cleaned['year'], errors='coerce')
        if 'value' in df_cleaned.columns:
            df_cleaned['value'] = df_cleaned['value'].str.replace(',', '.').astype('float')
        
        # –®–∞–≥ 4: –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–ø–µ—Ä–µ—Ö–æ–¥ –æ—Ç Dask –∫ Pandas)
        logger.info("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ DataFrame...")
        df_final = df_cleaned.compute()
        
        # –®–∞–≥ 5: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        logger.info(f"–ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(df_final):,}")
        logger.info("–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤...")
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        df_final = df_final[df_final['value'] != 9999999999.0]
        
        # –£–¥–∞–ª—è–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –≤–µ—â–µ—Å—Ç–≤–∞
        df_final = df_final[~df_final['substance'].isin(['CD', 'ND'])]
        
        logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df_final):,}")
        logger.info(f"–ì–æ–¥—ã –≤ –¥–∞–Ω–Ω—ã—Ö: –æ—Ç {df_final['year'].min()} –¥–æ {df_final['year'].max()}")
        logger.info(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤–µ—â–µ—Å—Ç–≤: {df_final['substance'].nunique()}")
        
        return df_final
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise

@task(name="create_database_tables")
def create_database_tables(
    df_final: pd.DataFrame, 
    db_path: str = db_path
) -> Tuple[int, dict]:
    """
    –ó–∞–¥–∞—á–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–æ–∑–¥–∞–Ω–Ω—ã–º —Ç–∞–±–ª–∏—Ü–∞–º
    """
    logger = get_run_logger()
    logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {db_path}")
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        stats = {}
        
        # ============================================================================
        # 1. –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ air_emissions
        # ============================================================================
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã air_emissions...")
        air_emissions_cols = ['section', 'code', 'substance', 'value', 'oktmo_code', 'year']
        air_emissions_df = df_final[air_emissions_cols].copy()
        air_emissions_df.to_sql('air_emissions', conn, if_exists='replace', index=False)
        stats['air_emissions'] = len(air_emissions_df)
        logger.info(f"–¢–∞–±–ª–∏—Ü–∞ 'air_emissions': {len(air_emissions_df):,} –∑–∞–ø–∏—Å–µ–π")
        
        # ============================================================================
        # 2. –¢–∞–±–ª–∏—Ü–∞ indicator_codes
        # ============================================================================
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã indicator_codes...")
        indicator_data = df_final[['code', 'indicator']].drop_duplicates()
        indicator_data = indicator_data.sort_values('code')
        indicator_data.to_sql('indicator_codes', conn, if_exists='replace', index=False)
        stats['indicator_codes'] = len(indicator_data)
        logger.info(f"–¢–∞–±–ª–∏—Ü–∞ 'indicator_codes': {len(indicator_data):,} –∑–∞–ø–∏—Å–µ–π")
        
        # ============================================================================
        # 3. –¢–∞–±–ª–∏—Ü–∞ substance_types
        # ============================================================================
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã substance_types...")
        substance_data = df_final[['substance', 'source_type']].drop_duplicates('substance')
        substance_data = substance_data.sort_values('substance')
        substance_data.to_sql('substance_types', conn, if_exists='replace', index=False)
        stats['substance_types'] = len(substance_data)
        logger.info(f"–¢–∞–±–ª–∏—Ü–∞ 'substance_types': {len(substance_data):,} –∑–∞–ø–∏—Å–µ–π")
        
        # ============================================================================
        # 4. –¢–∞–±–ª–∏—Ü–∞ location_codes
        # ============================================================================
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã location_codes...")
        location_cols = ['oktmo_code', 'municipal_formation', 'municipal_district', 'region']
        location_data = df_final[location_cols].drop_duplicates()
        location_data = location_data.sort_values('oktmo_code')
        location_data.to_sql('location_codes', conn, if_exists='replace', index=False)
        stats['location_codes'] = len(location_data)
        logger.info(f"–¢–∞–±–ª–∏—Ü–∞ 'location_codes': {len(location_data):,} –∑–∞–ø–∏—Å–µ–π")
        
        # ============================================================================
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤
        # ============================================================================
        logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤...")
        indexes = [
            ("air_emissions", "idx_air_year", "year"),
            ("air_emissions", "idx_air_code", "code"),
            ("air_emissions", "idx_air_substance", "substance"),
            ("air_emissions", "idx_air_section", "section"),
            ("air_emissions", "idx_air_oktmo", "oktmo_code"),
            ("indicator_codes", "idx_indicator_code", "code"),
            ("substance_types", "idx_substance", "substance"),
            ("location_codes", "idx_location_oktmo", "oktmo_code"),
        ]
        
        for table, idx_name, column in indexes:
            try:
                cursor.execute(f"CREATE INDEX IF NOT EXISTS {idx_name} ON {table}({column});")
                logger.debug(f"–ò–Ω–¥–µ–∫—Å {idx_name} —Å–æ–∑–¥–∞–Ω")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞ {idx_name}: {e}")
        
        conn.commit()
        conn.close()
        
        total_records = sum(stats.values())
        logger.info(f"–í—Å–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {total_records:,}")
        
        return total_records, stats
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–∞–±–ª–∏—Ü: {e}")
        raise

@task(name="validate_database")
def validate_database(db_path: str = db_path) -> dict:
    """
    –ó–∞–¥–∞—á–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü
    """
    logger = get_run_logger()
    logger.info(f"–í–∞–ª–∏–¥–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {db_path}")
    
    try:
        validation_results = {}
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;")
        tables = cursor.fetchall()
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ç–∞–±–ª–∏—Ü: {len(tables)}")
        
        for table in tables:
            table_name = table[0]
            
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
            cursor.execute(f"SELECT COUNT(*) FROM {table_name};")
            count = cursor.fetchone()[0]
            
            # –°—Ç–æ–ª–±—Ü—ã
            cursor.execute(f"PRAGMA table_info({table_name});")
            columns = cursor.fetchall()
            column_names = [col[1] for col in columns]
            
            # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö (–ø–µ—Ä–≤—ã–µ 2 —Å—Ç—Ä–æ–∫–∏)
            cursor.execute(f"SELECT * FROM {table_name} LIMIT 2;")
            sample = cursor.fetchall()
            
            validation_results[table_name] = {
                'row_count': count,
                'columns': column_names,
                'sample': sample
            }
            
            logger.info(f"üìä {table_name}: {count:,} –∑–∞–ø–∏—Å–µ–π, —Å—Ç–æ–ª–±—Ü—ã: {', '.join(column_names[:3])}...")
            
            if count == 0:
                logger.warning(f"–¢–∞–±–ª–∏—Ü–∞ {table_name} –ø—É—Å—Ç–∞—è!")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏
        logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏...")
        
        checks = [
            ("–ü—Ä–æ–≤–µ—Ä–∫–∞ indicator_codes", 
             "SELECT COUNT(DISTINCT code) FROM air_emissions WHERE code NOT IN (SELECT code FROM indicator_codes)"),
            ("–ü—Ä–æ–≤–µ—Ä–∫–∞ substance_types",
             "SELECT COUNT(DISTINCT substance) FROM air_emissions WHERE substance NOT IN (SELECT substance FROM substance_types)"),
            ("–ü—Ä–æ–≤–µ—Ä–∫–∞ location_codes",
             "SELECT COUNT(DISTINCT oktmo_code) FROM air_emissions WHERE oktmo_code NOT IN (SELECT oktmo_code FROM location_codes)")
        ]
        
        for check_name, query in checks:
            try:
                cursor.execute(query)
                missing = cursor.fetchone()[0]
                if missing == 0:
                    logger.info(f"{check_name}: –≤—Å–µ —Å—Å—ã–ª–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã")
                else:
                    logger.warning(f"{check_name}: {missing} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Å—Å—ã–ª–æ–∫")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ {check_name}: {e}")
        
        conn.close()
        
        return validation_results
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise

# ============================================================================
# FLOW (–û—Å–Ω–æ–≤–Ω–æ–π –ø–æ—Ç–æ–∫)
# ============================================================================

@flow(
    name="Air Quality ETL Pipeline",
    description="–ü–æ–ª–Ω—ã–π ETL-–ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –æ –≤—ã–±—Ä–æ—Å–∞—Ö –≤ –∞—Ç–º–æ—Å—Ñ–µ—Ä—É",
    task_runner=DaskTaskRunner(),
    version="1.0.0",
    log_prints=True
)
def air_quality_etl_flow(
    data_file: str = csv_path,
    db_file: str = db_path,
    run_validation: bool = True
) -> dict:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π ETL Flow –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –æ –∫–∞—á–µ—Å—Ç–≤–µ –≤–æ–∑–¥—É—Ö–∞
    
    Args:
        data_file: –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É CSV —Ñ–∞–π–ª—É
        db_file: –ü—É—Ç—å –∫ SQLite –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        run_validation: –ó–∞–ø—É—Å–∫–∞—Ç—å –ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    """
    logger = get_run_logger()
    logger.info("–ó–∞–ø—É—Å–∫ Air Quality ETL Pipeline")
    logger.info(f"–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {data_file}")
    logger.info(f"–í—ã—Ö–æ–¥–Ω–∞—è –ë–î: {db_file}")
    logger.info(f"–í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∑–∞–¥–∞—á–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
    raw_data = extract_data(data_file)
    transformed_data = transform_data(raw_data)
    total_records, table_stats = create_database_tables(transformed_data, db_file)
    
    if run_validation:
        validation_results = validate_database(db_file)
    else:
        validation_results = {}
    
    # –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    summary = {
        'timestamp': datetime.now().isoformat(),
        'input_file': data_file,
        'database_file': db_file,
        'total_records_processed': total_records,
        'table_statistics': table_stats,
        'validation_results': validation_results,
        'status': 'COMPLETED'
    }
    
    logger.info("=" * 50)
    logger.info("–ò–¢–û–ì–ò –í–´–ü–û–õ–ù–ï–ù–ò–Ø:")
    logger.info("=" * 50)
    logger.info(f"–°—Ç–∞—Ç—É—Å: {summary['status']}")
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {total_records:,}")
    logger.info(f"–¢–∞–±–ª–∏—Ü—ã —Å–æ–∑–¥–∞–Ω—ã: {', '.join(table_stats.keys())}")
    logger.info(f"–í—Ä–µ–º—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("ETL-–ø–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    
    return summary

# ============================================================================
# –ó–ê–ü–£–°–ö –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø
# ============================================================================

if __name__ == "__main__":
    # –í–∞—Ä–∏–∞–Ω—Ç 1: –ü—Ä—è–º–æ–π –∑–∞–ø—É—Å–∫ flow
    result = air_quality_etl_flow(
        data_file=csv_path,
        db_file=db_path,
        run_validation=True
    )
    
    print("\n" + "="*60)
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–´–ü–û–õ–ù–ï–ù–ò–Ø:")
    print("="*60)
    print(f"–°—Ç–∞—Ç—É—Å: {result['status']}")
    print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {result['total_records_processed']:,}")
    for table, count in result['table_statistics'].items():
        print(f"  {table}: {count:,} –∑–∞–ø–∏—Å–µ–π")